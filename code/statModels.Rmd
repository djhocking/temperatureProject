Overall approach: 
1) Define bps and slopes by segment for each site/year combo in temperatureSegmentedBreakPointAnalysis.Rmd. 
2) Model slopes for each segment (2=sp-sum, 3=sum-autumn) as a function of airTemp and fixed covariates. This gets predicted water temp as a function of airTemp and covariates, but does not identify bps.
3) Model bps as a fucntion of covariates including swe for bp1.
4) Predict water temp as function of airTemp and covariates between bps for each prediction site
5) Summarize data for slopes btw bps

Note: run temperatureSegmentedBreakPointAnalysis.Rmd before running this script

```{r load libs}
rm(list=ls())

library(ggplot2)
library(relaimpo)
library(lme4)
library(DataCombine) # for the slide function
library(plyr)
library(reshape)
library(ggmap)
library(foreign)
library(maptools)
library(gridExtra)
library(zoo)

#setwd('/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/')
setwd('C:/KPONEIL/GitHub/projects/temperatureProject/')

baseDir <- 'C:/Users/Admin_7600/Documents/Hocking/'
baseDir <- 'C:/KPONEIL/GitHub/projects/temperatureProject/'
#baseDir <- '/Users/Dan/Documents/Research/Stream_Climate_Change/temperatureProject/'

dataInDir <- paste0(baseDir, 'dataIn/')
dataOutDir <- paste0(baseDir, 'dataOut/')
graphsDir <- paste0(baseDir, 'graphs/')

source(paste0(baseDir, 'code/functions/temperatureModelingFunctions.R'))
```

Which agencies do you want to pull data from?
```{r Define data sources and other options}

#If removeSelectSites = TRUE, then the file with the list of sites needs to be specified.
removeSelectSites <- T
sitesToRemove <- paste0(baseDir, 'dataIn/sitesToRemoveAllNE.csv')

#Do you want all of the plots made?
makePlots <- F

#Use validation?
validate=F
  
#If validating:
  # Choose fraction of total # of sites:
  validateFrac <- 0.1

  #Do you want to create bias maps? (Internet connection required)
  createBiasMaps <- F

#Data source agencies?
CTDEP  <- T
MAFW   <- T
MAUSGS <- T
NHFG   <- T
NHDES  <- T
USFS   <- T
VTFWS  <- T
MEDMR  <- F
MTUSGSYellowstone <- F
MTUSGSGlacier <- F

#global vars
dpiIn <- 400

```

```{r load data} 

#Set up data list.
sourceChoice <- list( CTDEP,   MAFW,   MAUSGS,   NHFG,   NHDES,   MEDMR,   USFS,   VTFWS,    MTUSGSYellowstone,   MTUSGSGlacier )
sourceNames  <- c   ('CTDEP', 'MAFW', 'MAUSGS', 'NHFG', 'NHDES', 'MEDMR', 'USFS', 'VTFWS',  'MTUSGSYellowstone', 'MTUSGSGlacier')

dataSource <- sourceNames[sourceChoice == T]

#sourceList <- paste0(paste0(dataSource, collapse = '_'))
sourceList <- paste0(paste0(dataSource, collapse = '_'), '_2014-04-24')

#Load "et" for the agencies:
load(paste0(dataOutDir, sourceList,  '/et_', sourceList, '.RData'))

#Pull duplicate columns so they doesn't get doubled up
et <- et[,-which(names(et) %in% c('Latitude', 'Longitude', 'StreamOrder', 'HUC_4', 'HUC_8.x', 'HUC_8.y', 'HUC_12', 'agency'))]

#Load in covariate data to merge into slopes df [no day data]
for ( i in 1:length(dataSource)){

  #Load covariate data to be merged into slopes df [no day data]
  load(paste0(dataInDir, dataSource[i], '/covariateData_', dataSource[i], '.RData')) #Fixed over time
  covariateData$agency <- paste(dataSource[i])
  if ( i == 1) {covs <- covariateData} else (covs <- rbind(covs, covariateData))
  
  #Load daymet climate data to be merged into et:
  load(paste0(dataInDir, dataSource[i], '/streamTempSitesObservedClimateData_', dataSource[i], '.RData')) 
  
  #Pull out the columns needed:
  masterData <- masterData[, c('site', 'year', 'dOY', 'date', 'dayl', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'prcp')]
  if ( i == 1) {newDay <- masterData} else ( newDay <- rbind(newDay, masterData) )
}

masterData    <- newDay
covariateData <- covs

#Merge climate data into main dataframe:
et <- merge(et, masterData, by = c('site', 'date', 'year', 'dOY'), all.x=T, sort = F )

et$flow <- NA
et$tAirMin <- et$tmin; et$tAirMax <- et$tmax

#Overwrite NaNs with NAs:
covariateData <- replace(covariateData, is.na(covariateData), NA)

#Make site a character string so the "merge" function works:
covariateData$site <- as.character(covariateData$site)

##For testing
#testSites <- c("MAUSGS_WB_JIMMY", "MAUSGS_SEC_45_DL", "MAUSGS_WEST_BROOK", "MAUSGS_SEC_6_DL", "MAUSGS_SEC_30_DL", "MAUSGS_WB_OBEAR", "MAUSGS_WB_MITCHELL")

#et <- et[et$site %in% testSites,]
#covariateData <- covariateData[covariateData$site %in% testSites,]

#=================================================================================================================
#Scale the variables used in the model. Some get log-scaled depending on their distribution over the sites:
#=================================================================================================================
# Standard scaling:
covariateData$LatitudeS              <- (covariateData$Latitude        - mean(covariateData$Latitude       , na.rm=T)) / sd(covariateData$Latitude       , na.rm=T)
covariateData$LongitudeS             <- (covariateData$Longitude       - mean(covariateData$Longitude      , na.rm=T)) / sd(covariateData$Longitude      , na.rm=T)
covariateData$ForestS                <- (covariateData$Forest          - mean(covariateData$Forest         , na.rm=T)) / sd(covariateData$Forest         , na.rm=T)
covariateData$BasinElevationMS       <- (covariateData$BasinElevationM - mean(covariateData$BasinElevationM, na.rm=T)) / sd(covariateData$BasinElevationM, na.rm=T)
covariateData$ReachSlopePCNTS        <- (covariateData$ReachSlopePCNT  - mean(covariateData$ReachSlopePCNT , na.rm=T)) / sd(covariateData$ReachSlopePCNT , na.rm=T)
covariateData$WetlandOrWaterS        <- (covariateData$WetlandOrWater  - mean(covariateData$WetlandOrWater , na.rm=T)) / sd(covariateData$WetlandOrWater , na.rm=T)

# Log scaling:
covariateData$AgricultureLS          <- (log(covariateData$Agriculture          + 0.001) - mean(log(covariateData$Agriculture          + 0.001), na.rm=T)) / sd(log(covariateData$Agriculture          + 0.001), na.rm=T)
covariateData$TotDASqKMLS            <- (log(covariateData$TotDASqKM            + 0.001) - mean(log(covariateData$TotDASqKM            + 0.001), na.rm=T)) / sd(log(covariateData$TotDASqKM            + 0.001), na.rm=T)
covariateData$SurficialCoarseCLS     <- (log(covariateData$SurficialCoarseC     + 1    ) - mean(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)) / sd(log(covariateData$SurficialCoarseC     + 1    ), na.rm=T)
covariateData$ImpoundmentsOpenSqKMLS <- (log(covariateData$ImpoundmentsOpenSqKM + 1    ) - mean(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)) / sd(log(covariateData$ImpoundmentsOpenSqKM + 1    ), na.rm=T)

et <- merge(et, covariateData, by = 'site', all.x=T, sort = F )

#====================================================================================================
#Remove slected site/years that have errors in breakpoint assignment (chosen via visual examination):
#====================================================================================================
if( removeSelectSites ) {
  
  removeSites <- read.csv(sitesToRemove)
  
  removeSites$site <- as.character(removeSites$site)
  
  et <-  et[!(et$site %in% removeSites$site & et$year %in% removeSites$year),]
}
#====================================================================================================

#Get BPs out of et
bp <- unique(et[,c('site','year','springBP','summerBP','fallBP')]  ) #, 'Latitude', 'Longitude'
bp <- bp[is.finite(bp$springBP) | is.finite(bp$summerBP) | is.finite(bp$fallBP),]
bp$site <- as.character(bp$site) #for merging
  
siteData <- merge( x = bp, y = covariateData, by = 'site', all.x=T )

# turn Inf to NA in bps
siteData[!is.finite(siteData$springBP),'springBP'] <- NA
siteData[!is.finite(siteData$summerBP),'summerBP'] <- NA
siteData[!is.finite(siteData$fallBP),'fallBP'] <- NA

# merge in count of days
obsBySiteYear <- ddply(et, .(site,year), summarize,count=length(!is.na(temp)))
siteData <- merge(x=siteData, y=obsBySiteYear, all.x=T)

```

```{r lag airTemp & prcp}
et <- et[order(et$count),] # just to make sure et is ordered for the slide function

# airTemp
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
et <- slide(et, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

# prcp
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
et <- slide(et, Var = "prcp", GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')

```

Left out to save time:
----------------------------------------------------------------------------------------------------------------------------------------------------------------
# 5-day mean of prcp 
siteYearCombos <- unique(et[,c('site','year')])

et$prcp5Day <- NA

window <- 5
for (i in 1:nrow(siteYearCombos)){

  print(c(i,as.character(siteYearCombos$site[i]),siteYearCombos$year[i],i/nrow(siteYearCombos)))
  
  currSite <- which(et$site == as.character(siteYearCombos$site[i]) & et$year == siteYearCombos$year[i] )

  #Need this so sites with very short records don't crash the loop.
  if(length(currSite) >= window){currMean <-  rollapply(et$prcp[currSite], width=window, fill=NA, mean, align = 'left')} else(currMean <- NA)
  
  et$prcp5Day[currSite] <- currMean
}
----------------------------------------------------------------------------------------------------------------------------------------------------------------

```{r check out data}
#pairs(~Latitude+Longitude+Forest+ Impervious+ Agriculture+ BasinElevationM+ ReachSlopePCNT+ TotDASqKM+ WetlandOrWater+ SurficialCoarseC,data=et)
#Latitude, Longitude, Forest, Impervious, Agriculture, BasinElevationM, ReachSlopePCT, TotDASqKM, WetlandOrWater, SurficialCoarseC

if(makePlots) {

  #Makes barcode looking plot of data records:
  #-------------------------------------------
  gTile <- 
  ggplot(siteData,aes(site,year,z=any(c(!is.na(springBP),!is.na(summerBP),!is.na(fallBP)))))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+
    theme_bw(base_size=20) + 
      theme(axis.text.x = element_blank())+
    geom_tile()
  
  ggsave( file=paste0(graphsDir, sourceList, '/gTile.png'), plot=gTile, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Colors by number of observations?
  #---------------------------------
  gTileHeat <- 
  ggplot(siteData,aes(site,year,z=count))+
    geom_tile(aes(fill=count))+
    scale_x_discrete('Site')+
    scale_y_continuous('Year')+ 
    theme(axis.text.x = element_blank())
  
  ggsave( file=paste0(graphsDir, sourceList,'/gTileHeat.png'), plot=gTileHeat, dpi=dpiIn , width=8,height=5, units='in' )

}

```

```{r scale daymet vars and split et into rising and falling segments}

#List the variables you want logged.
logVariables <- c('swe', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3')# 'prcp5Day'

#List the variables you want scaled. (If you want the logged ones scaled include them as well)
scaleVariables <- c('swe', 'dayl', 'srad', 'prcp', 'prcpLagged1', 'prcpLagged2', 'prcpLagged3')# 'prcp5Day'

# Run the scaling funtion:
logScaledData <- loggAndScaleDaymet(et, logVariables, scaleVariables)

# Rename output:
et  <- logScaledData[[1]] # Main dataframe
et2 <- logScaledData[[2]] # Segment 2
et3 <- logScaledData[[3]] # Segment 3
```

Models for rising & falling segments
------------------------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "mX"
   2) Assign the description of the model and name it: "dX"
   3) Add new models to the model lists ("segModels" and "segModelsDescriptions")
  
```{r regression temp~airTemp+...}
#This section explores modeling temperature using fixed covariate data and time series of climate data.
# Models are created by segment (warming and cooling).

#=================================================================================================================
#                                         Define the Segment Models:
#=================================================================================================================

#Model 1:
d1 <- 'Air temp & lags only'
#---------------------------
m1 <- 'temp~airTemp+airTempLagged1+airTempLagged2'


#Model 2:

d2 <- 'All main effects'
#-----------------------
m2 <-'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS'


#Model 3:
d3 <- 'All two-way interactions'
#-------------------------------
m3 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS)^2'

#Model 4:
d4 <- 'All main effects (add prcp)'
#----------------------------------
m4 <- 'temp~airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS'

#Model 5:
d5 <- 'All two-way interactions (add prcp)'
#------------------------------------------
m5 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS)^2'

#Model 6:
d6 <- 'All two-way interactions (prcp Lag +1)'
#---------------------------------------------
m6 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged1LS)^2'

#Model 7:
d7 <- 'All two-way interactions (prcp Lag +2)'
#---------------------------------------------
m7 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged2LS)^2'


#Model 8:
d8 <- 'All two-way interactions (prcp Lag +3)'
#---------------------------------------------------------------------
m8 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLagged3LS)^2'


#Model 9:
d9 <- 'All two-way interactions (prcp 5-day mean)'
#-------------------------------------------------
m9 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcp5DayLS)^2'

#Model 10:
d10 <- 'All two-way interactions (prcp  + prcpLags 1, 2, & 3)'
#------------------------------------------------------
m10 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged1LS + prcpLagged2LS + prcpLagged3LS)^2'

#Model 11:
d11 <- 'All two-way interactions (prcp + prcp Lag +1)'
#---------------------------------------------
m11 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged1LS)^2'

#Model 12:
d12 <- 'All two-way interactions (prcp + prcp Lag +2)'
#---------------------------------------------
m12 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLagged2LS)^2'

#Model 13:
d13 <- 'All two-way interactions (prcp + prcp Lag +3)'
#---------------------------------------------------------------------
m13 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcpLagged3LS)^2'

#Model 14:
d14 <- 'All two-way interactions (prcp + prcp 5-day mean)'
#-------------------------------------------------
m14 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS + prcp5DayLS)^2'

#Model 15:
d15 <- 'All two-way interactions (all prcp terms)'
#------------------------------------------------------
m15 <- 'temp~(airTemp+airTempLagged1+airTempLagged2+
              LatitudeS+LongitudeS+
              ForestS+ AgricultureLS+ 
              BasinElevationMS+ ReachSlopePCNTS+ TotDASqKMLS+ 
              WetlandOrWaterS+ SurficialCoarseCLS+ImpoundmentsOpenSqKMLS+ 
              daylS + sradS + sweLS + prcpLS  + prcpLagged1LS + prcpLagged2LS + prcpLagged3LS + prcp5DayLS)^2'

segModels             <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15)
segModelsDescriptions <- list(d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)
```

# If you already know the model you want to run then run this code with the model entered. Otherwise skip to next section to compare models.
```{r Already know model}

segModels             <- list(m10)
segModelsDescriptions <- list(d10)

```

```{r Run and validate models}
#=================================================================================================================
#                                                    Segment 2
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS2 <- sample(unique(et2$site), size = validateFrac*length(unique(et2$site)))
valData2    <- et2[et2$site %in% randomSitesS2,]

mS2  <- list()
vmS2 <- list()

for (i in 1:length(segModels)){
  print(i/length(segModels))  
  
  mS2[[i]] <- lm(as.formula(segModels[[i]]), data=et2)
  
  if (validate){
    vmS2[[i]] <- validateModel( mS2[[i]], et2, valData2 )
    if(createBiasMaps) {makeBiasMap(vmS2[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS2) ){   
      a <- data.frame(vmS2[[i]]$means, extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
  
} else ( for ( i in 1:length(mS2) ){   
      a <- data.frame(extractAIC(mS2[[i]])[1], extractAIC(mS2[[i]])[2], summary(mS2[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS2 <- a} else ( modelMetricsS2 <- rbind(modelMetricsS2, a))
  }# for loop
  
)

# Label comparison table
if(validate) {colnames(modelMetricsS2) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')} else (colnames(modelMetricsS2) <- c('AIC.df', 'AIC', 'Sigma', 'Model Description'))
rownames(modelMetricsS2) <-   paste0( 'm', seq(from =1, to = length(mS2), by = 1), 'S2')  

modelMetricsS2

#=================================================================================================================
#                                                    Segment 3
#=================================================================================================================
#Validate for a random subset of sites:

randomSitesS3 <- sample(unique(et3$site), size = validateFrac*length(unique(et3$site)))
valData3    <- et3[et3$site %in% randomSitesS3,]

mS3  <- list()
vmS3 <- list()

for (i in 1:length(segModels)){
  print(i/length(segModels))
  
  mS3[[i]] <- lm(as.formula(segModels[[i]]), data=et3)
  
  if (validate){
    vmS3[[i]] <- validateModel( mS3[[i]], et3, valData3 )
    if(createBiasMaps) {makeBiasMap(vmS3[[i]])}
  }
}

#-------------------
#Compare the models:
#-------------------
if(validate){
  for ( i in 1:length(vmS3) ){   
      a <- data.frame(vmS3[[i]]$means, extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop

} else ( for ( i in 1:length(mS3) ){   
      a <- data.frame(extractAIC(mS3[[i]])[1], extractAIC(mS3[[i]])[2], summary(mS3[[i]])$sigma, segModelsDescriptions[[i]])

      if( i == 1 ) { modelMetricsS3 <- a} else ( modelMetricsS3 <- rbind(modelMetricsS3, a))
  }# for loop
)

# Label comparison table
if(validate) {colnames(modelMetricsS3) <- c('meanRMSE', 'meanBiasMean', 'meanBiasSD', 'AIC.df', 'AIC', 'Sigma', 'Model Description')} else (colnames(modelMetricsS3) <- c('AIC.df', 'AIC', 'Sigma', 'Model Description'))
rownames(modelMetricsS3) <-   paste0( 'm', seq(from =1, to = length(mS3), by = 1), 'S3')  


modelMetricsS3
```

```{r Save model metrics}
#=================================================================================================================
#                                                   Save the model metrics
#=================================================================================================================

# Want to save the metrics but it's a "list" so can't be a .csv for some reason. Savings as .RData for now.
if(validate){
  segmentModelMetrics <- rbind(modelMetricsS2, modelMetricsS3)
  save(segmentModelMetrics, file=paste0(dataOutDir, sourceList,'/segmentModelMetrics.RData' ) )
}
```

```{r Choose models to use and predict stream temp values}
#After model comparison, choose which model to use to predict:

predictionModelNum <- 1

finalModelS2  <- mS2 [[predictionModelNum]]; finalModelS3   <- mS3 [[predictionModelNum]]
if(validate) {finalValModS2 <- vmS2[[predictionModelNum]]; finalValModS3  <- vmS3[[predictionModelNum]]}

# Predict stream temperatures:
et2[,c('pred', 'lowr', 'upr')] <- predict (finalModelS2, newdata=et2, interval = "confidence") #default confidence interval (CI) = 95%
et3[,c('pred', 'lowr', 'upr')] <- predict (finalModelS3, newdata=et3, interval = "confidence") #default confidence interval (CI) = 95%

# Predict air/stream temperature slopes:
et2Slopes <- ddply( et2, .(site), summarize, slopeSeg2=coef(lm(pred ~ airTemp))[2])
et3Slopes <- ddply( et3, .(site), summarize, slopeSeg3=coef(lm(pred ~ airTemp))[2])

siteData <- merge(siteData, et2Slopes, by = 'site', all.x = T, sort = F)
siteData <- merge(siteData, et3Slopes, by = 'site', all.x = T, sort = F)

```

```{r graphs of predicted/obs etc}

# In this section we want to add in coloring by state or agency
#   Use:     geom_point(aes(color = factor(site))) +

#Also: "geom_text(aes(label=site))" for looking at specific sites

# Plot all observed vs predicted data:
gPredObs <- 
  ggplot(et3,aes(pred,temp))+
  geom_point(size=0.5) +
    scale_x_continuous(expression(paste("Predicted water temperature (",degree, "C)", sep = "")))+
    scale_y_continuous(expression(paste("Observed water temperature (",degree, "C)", sep = "")))+
  theme_bw(base_size=20) +
  geom_abline(intercept=0,slope=1,color='white')

ggsave( file=paste0(graphsDir, sourceList,'/predObs.png'), plot=gPredObs, dpi=dpiIn , width=8,height=5, units='in' )

summary(lm(temp~pred,et3))

if(validate){
  
  #Plot the mean bias from the validation:
  gMeanBias <- ggplot( finalValModS2$v, aes(biasMean)) +
    geom_histogram()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('Frequency')
  
  #Plot the std dev of the bias from the validation:
  gSDBias <- ggplot( finalValModS2$v, aes(biasMean,biasSD)) +
    geom_point()+
    scale_x_continuous('Mean bias for each site/year') +
    scale_y_continuous('SD bias for each site/year')
  
  gValStats <- arrangeGrob( gMeanBias, gSDBias, ncol=1 )
  
  ggsave( file=paste0(graphsDir, sourceList,'/modelValidation.png'), plot=gValStats, dpi=dpiIn , width=8,height=5, units='in' )

}
```

Models for breakpoints
----------------------
When adding new models:
   1) Follow the structure of previously defined models and name it: "bpmX"
   2) Assign the description of the model and name it: "bpdX"
   3) Add new models to the model lists ("bdModels" and "bpModelsDescriptions")

```{r regression bp~+...}
#This section models the breakpoints as a function of fixed covariates.

#=================================================================================================================
#                                          Define the structure of the breakpoint models
#=================================================================================================================

#Breakpoint Model 1:
#-------------------
bpd1 <- 'No interactions'
bpm1 <- '~LatitudeS + LongitudeS + ForestS + AgricultureLS +  BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS +(1|year)'

#Breakpoint Model 2:
#-------------------
bpd2 <- 'Full interactions'
bpm2 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS )^2 + (1|year)'

#Breakpoint Model 3: 
#-------------------
bpd3 <- 'Full interactions. Impoundments added.'
bpm3 <-  '~ ( LatitudeS + LongitudeS + ForestS + AgricultureLS + BasinElevationMS + ReachSlopePCNTS + TotDASqKMLS + WetlandOrWaterS + SurficialCoarseCLS + ImpoundmentsOpenSqKMLS )^2 + (1|year)'

# List all of the models for reference later:
bpModels <- list(bpm1, bpm2, bpm3)
bpModelsDescriptions <- list(bpd1, bpd2, bpd3)

#=================================================================================================================
#                                               Breakpoint Models
#=================================================================================================================

bp1mods <- list()
bp2mods <- list()
bp3mods <- list()

for ( i in 1:length(bpModels)){
  
  # Breakpoint Models
  bp1mods[[i]] <- lmer(as.formula(paste('springBP', bpModels[[i]])), data=siteData)
  bp2mods[[i]] <- lmer(as.formula(paste('summerBP', bpModels[[i]])), data=siteData)
  bp3mods[[i]] <- lmer(as.formula(paste('fallBP'  , bpModels[[i]])), data=siteData)
  
  # Breakpoint Model AICs
  spr <- data.frame(extractAIC(bp1mods[[i]])[1], extractAIC(bp1mods[[i]])[2], bpModelsDescriptions[[i]])
  names(spr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { sprBPModelMetrics <- spr} else ( sprBPModelMetrics <- rbind(sprBPModelMetrics, spr))

  smr <- data.frame(extractAIC(bp2mods[[i]])[1], extractAIC(bp2mods[[i]])[2], bpModelsDescriptions[[i]])
  names(smr) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { smrBPModelMetrics <- smr} else ( smrBPModelMetrics <- rbind(smrBPModelMetrics, smr))

  fal <- data.frame(extractAIC(bp3mods[[i]])[1], extractAIC(bp3mods[[i]])[2], bpModelsDescriptions[[i]])
  names(fal) <- c('df', 'AIC', 'Model Description')
  if( i == 1 ) { falBPModelMetrics <- fal} else ( falBPModelMetrics <- rbind(falBPModelMetrics, fal)) 
}

sprBPModelMetrics
smrBPModelMetrics
falBPModelMetrics

#Checked "AIC" vs "extractAIC" on these models and they give the same values.
```

```{r Choose models to use and predict breakpoints}

bpModelNum <- 3

finalModBP1 <- bp1mods[[bpModelNum]]
finalModBP2 <- bp2mods[[bpModelNum]]
finalModBP3 <- bp3mods[[bpModelNum]]

# not sure why need this [allow.new.levels=T] but throws an error otherwise
#may be because year is in the df

#BP1
siteData$bp1Pred <- predict(finalModBP1,newdata=siteData,allow.new.levels=T)
siteData$bp1PredAvgYear <- predict(finalModBP1,newdata=siteData,REform=NA)

#BP2
siteData$bp2Pred <- predict(finalModBP2,newdata=siteData,allow.new.levels=T)
siteData$bp2PredAvgYear <- predict(finalModBP2,newdata=siteData,REform=NA)

#BP3
siteData$bp3Pred <- predict(finalModBP3,newdata=siteData,allow.new.levels=T)
siteData$bp3PredAvgYear <- predict(finalModBP3,newdata=siteData,REform=NA)

#Synchronized Range
siteData$bp1bp3 <- siteData$bp3Pred - siteData$bp1Pred
siteData$bp1bp3AvgYear <- siteData$bp3PredAvgYear - siteData$bp1PredAvgYear

save(siteData,file=paste0(dataOutDir, sourceList,'/siteDataWBPs_', sourceList, '.RData'))
```

```{r Predicted BP graphs}

if ( makePlots ) {

  #Predicted vs observed spring BP:
  #--------------------------------
  gObsPredBP1 <- 
  ggplot(siteData[siteData$springBP>25,], aes(bp1Pred,springBP))+
    geom_point(aes(color = agency))+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted spring breakpoint")+
    scale_y_continuous("Observed spring breakpoint")+
    theme_bw(base_size=20)
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP1_.png'), plot=gObsPredBP1, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed summer BP:
  #--------------------------------
  #need to look into 2008, low observed values
  gObsPredBP2 <- 
  ggplot(siteData[siteData$summerBP>180&siteData$summerBP<240&siteData$year!=2008,], aes(bp2Pred,summerBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted summer breakpoint")+
    scale_y_continuous("Observed summer breakpoint")+
    theme_bw(base_size=20) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP2_.png'), plot=gObsPredBP2, dpi=dpiIn , width=8,height=5, units='in' )
  
  #Predicted vs observed fall BP:
  #------------------------------
  gObsPredBP3 <- 
  ggplot(siteData, aes(bp3Pred,fallBP))+
    geom_point()+
    geom_abline(intercept=0,slope=1)+
    scale_x_continuous("Predicted fall breakpoint")+
    scale_y_continuous("Observed fall breakpoint")+
    theme_bw(base_size=20)#+facet_wrap(~year) 
  
  ggsave( file=paste0(graphsDir, sourceList,'/gObsPredBP3_.png'), plot=gObsPredBP3, dpi=dpiIn , width=8,height=5, units='in' )

}
```








Predict values for selected catchements
1) Predict breakpoints for UpstreamStats
2) Merge bps into Daymet files
3) Identify segements in Daymet files
3) Predict water temp for segments 2,3

```{r Define prediction region, year, covariates used, and catchments.}

#Pick the area you want to predict for. This is done by selection of daymet tiles:
# See Map Here: http://daymet.ornl.gov/sites/default/files/images/Tiles_on_LCC_projection_300dpi_labels.jpg

DaymetTiles <- c(11754, 11755, 11934, 11935, 12114, 12115)

Year <- 2010

#Read in NHD catchments you want to predict for. This should fall within the boundaries of the daymet tiles above.
proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"

catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/CTRiverStates_NHDCatchment.shp", proj4string=CRS(proj4.NHD))

features <- catchments$FEATUREID

#Load the observed covariate data:
load(paste0(dataInDir, 'NENY_CovariateData_2014-04-28.RData'))

#Scale the covariates for prediction to match the model inputs:
#--------------------------------------------------------------
#Normal scaling:
UpstreamStats$LatitudeS        <- (UpstreamStats$Latitude        - mean(UpstreamStats$Latitude        ,na.rm=T)) /sd(UpstreamStats$Latitude        ,na.rm=T)
UpstreamStats$LongitudeS       <- (UpstreamStats$Longitude       - mean(UpstreamStats$Longitude       ,na.rm=T)) /sd(UpstreamStats$Longitude       ,na.rm=T)
UpstreamStats$ForestS          <- (UpstreamStats$Forest          - mean(UpstreamStats$Forest          ,na.rm=T)) /sd(UpstreamStats$Forest          ,na.rm=T)
UpstreamStats$BasinElevationMS <- (UpstreamStats$BasinElevationM - mean(UpstreamStats$BasinElevationM ,na.rm=T)) /sd(UpstreamStats$BasinElevationM ,na.rm=T)
UpstreamStats$ReachSlopePCNTS  <- (UpstreamStats$ReachSlopePCNT  - mean(UpstreamStats$ReachSlopePCNT  ,na.rm=T)) /sd(UpstreamStats$ReachSlopePCNT  ,na.rm=T)
UpstreamStats$WetlandOrWaterS  <- (UpstreamStats$WetlandOrWater  - mean(UpstreamStats$WetlandOrWater  ,na.rm=T)) /sd(UpstreamStats$WetlandOrWater  ,na.rm=T)

#Log scaling:
UpstreamStats$AgricultureLS          <- (log(UpstreamStats$Agriculture          + 0.001) - mean(log(UpstreamStats$Agriculture         + 0.001) ,na.rm=T)) / sd(log(UpstreamStats$Agriculture          + 0.001), na.rm=T)
UpstreamStats$TotDASqKMLS            <- (log(UpstreamStats$TotDASqKM            + 0.001) - mean(log(UpstreamStats$TotDASqKM            +0.001) ,na.rm=T)) / sd(log(UpstreamStats$TotDASqKM            + 0.001), na.rm=T)
UpstreamStats$SurficialCoarseCLS     <- (log(UpstreamStats$SurficialCoarseC     + 1    ) - mean(log(UpstreamStats$SurficialCoarseC     + 1   ) ,na.rm=T)) / sd(log(UpstreamStats$SurficialCoarseC     + 1    ), na.rm=T)
UpstreamStats$ImpoundmentsOpenSqKMLS <- (log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ) - mean(log(UpstreamStats$ImpoundmentsOpenSqKM +1    ) ,na.rm=T)) / sd(log(UpstreamStats$ImpoundmentsOpenSqKM + 1    ), na.rm=T)

```

```{r predicted values for select catchments}

#Here "UpstreamStatsCT" has become "predictionStats" and "CTday2010" has become "FullRecord".

#Select the prediction covariates to index:
predictionCovs <- c('FEATUREID', 'LatitudeS','LongitudeS','ForestS', 'AgricultureLS','BasinElevationMS','ReachSlopePCNTS', 'TotDASqKMLS', 'WetlandOrWaterS','SurficialCoarseCLS', 'ImpoundmentsOpenSqKMLS')

predictionStats <- UpstreamStats[ ,names(UpstreamStats) %in% c(predictionCovs, "StreamOrder")]

rm(UpstreamStats, LocalStats)

predictionStats$year <- Year # for bp predictions

# predict bps
#REform=NA uses no REs. Default is to use all REs (year in our case)
predictionStats$bp1 <- predict(finalModBP1,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp2 <- predict(finalModBP2,newdata=predictionStats, allow.new.levels = T)
predictionStats$bp3 <- predict(finalModBP3,newdata=predictionStats, allow.new.levels = T)

predictionStats$bp1bp3 <- predictionStats$bp3 - predictionStats$bp1
##########################

for ( i in 1:length(DaymetTiles)){
  
  print(i)
  
  # Read in daily data for tile:
  # ============================
  setwd("C:/KPONEIL/GitHub/projects/temperatureProject/dataIn/DaymetClimateData")
  
  load(paste0('NHD_DaymetTile_' , DaymetTiles[i], '_', Year, '.RData'))
  
  # Index the prediction data that matches the Daymet tile:
  predictionStatsTile <- predictionStats[predictionStats$FEATUREID %in% FullRecord$FEATUREID, ]

  # Trim the Daymet tile dataframes to the polygon of catchments you are interested in:
  if(exists('features')){predictionStatsTile <- predictionStatsTile[predictionStatsTile$FEATUREID %in% features, ]}
  if(exists('features')){FullRecord <- FullRecord[FullRecord$FEATUREID %in% features,]}
  
  # Renaming:
  names(FullRecord         )[names(FullRecord         ) == 'FEATUREID'] <- 'site'
  names(predictionStatsTile)[names(predictionStatsTile) == 'FEATUREID'] <- 'site' 
    
  # Merge in bps and assign segments. Do here to make FullRecord smaller:
  # =====================================================================
  FullRecord <- merge( x=FullRecord, y=predictionStatsTile, all.x=T, by = c('site', 'year' ) )
  
  #limit FullRecord to between bp1 and bp3 and assign segments
  FullRecord <- FullRecord[FullRecord$dOY > FullRecord$bp1 & 
                           FullRecord$dOY < FullRecord$bp3,]
  
  FullRecord$segment <- ifelse( FullRecord$dOY > FullRecord$bp1 & 
                               FullRecord$dOY <= FullRecord$bp2, 2,
                        ifelse( FullRecord$dOY > FullRecord$bp2 &
                               FullRecord$dOY <= FullRecord$bp3, 3, NA))
  
  
  # Get rid of site/segment combos with all low # of obs:
  # =====================================================
  counts     <- ddply( FullRecord, .(site,segment), summarize, count=length(na.omit(airTemp)))
  FullRecord <- merge( x=FullRecord, y=counts, all.x=T )
  FullRecord <- FullRecord[ FullRecord$count > 3, ]

  # Do the same for prcp. Need to look into why there are NAs
  counts1    <- ddply( FullRecord, .(site,segment), summarize, count1=length(na.omit(prcp)))
  FullRecord <- merge( x=FullRecord, y=counts1, all.x=T )
  FullRecord <- FullRecord[ FullRecord$count1 > 3, ]
  
  
  # Lag select daymet variables: (slow for big datasets)
  # ============================
  FullRecord <- FullRecord[order(FullRecord$site,FullRecord$dOY),] # just to make sure FullRecord is ordered for the slide function
  
  # airTemp
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
  FullRecord <- slide(FullRecord, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

  # prcp
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
  FullRecord <- slide(FullRecord, Var = "prcp",    GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')
  
  # Calculate 5-day mean of prcp:
  # =============================
  if('prcp5DayLS' %in% variable.names(finalModelS2) | 'prcp5DayLS' %in% variable.names(finalModelS3) ) {
  
    FullRecord$prcp5Day <- NA
    
    loopSites <- unique(FullRecord$site)
    
    window <- 5
    for (i in 1:length(loopSites)){
    
      currSite <- which(FullRecord$site == loopSites[i])
  
      FullRecord$prcp5Day[currSite] <- rollapply(FullRecord$prcp[currSite], width=window, fill=NA, mean, align = 'left')
    
      if(i/100 == round(i/100)) {print(paste0(i/length(loopSites), '% done 5-day averaging prcp'))}
    }
  }
    
  # Log scale the daymet variables:
  # ===============================
  
  # Only scale the variables that made it into the model:
  predLogVars   <- names(FullRecord)[names(FullRecord) %in% logVariables  ]
  predScaleVars <- names(FullRecord)[names(FullRecord) %in% scaleVariables]
  
  # Scale and log the same variables as the observed data:
  logScaledPreds <- loggAndScaleDaymet(FullRecord, predLogVars, predScaleVars)
  
  FullRecord  <- logScaledPreds[[1]]
  FR2 <- logScaledPreds[[2]]
  FR3 <- logScaledPreds[[3]]
  
  # Predict  Summer Tmax and its CI:
  # ================================
  predictionStatsTile <- predictSummerTMax(FR2, FR3, predictionStatsTile)
  
  # Predict daily water temp
  # ========================
  # Note: running the 'if else' script on the full dataframe runs into memory limitations. This is the reason for the split.
  FR2$predTemp <-predict(finalModelS2,FR2)
  FR3$predTemp <- predict(finalModelS3,FR3)
  
  # Join to over-write FullRecord: (Ok because we already pulled data not in segments 2 or 3)
  # ==============================
  FullRecord <- rbind(FR2, FR3) 
  
  # Estimate slopes of air/water for each site:
  # ===========================================
  predictionStatsTile <- predictSlopes(FullRecord, predictionStatsTile)
  
  # Create a master dataframe of all predictions:
  # =============================================
  TempPreds <- predictionStatsTile[,c('site', 'bp1', 'bp2', 'bp3', 'bp1bp3', 'slopeSeg2', 'slopeSeg2CI',  'slopeSeg3', 'slopeSeg3CI', 'summerMax', 'summerMaxCI')]
    
  if ( i == 1 ) {Predictions <- TempPreds}  else (Predictions <- rbind(Predictions, TempPreds))

  
  

  #Creates folders for graphs if they don't exist:
  #-----------------------------------------------
#  subGraphsDir  <- paste0(graphsDir,  sourceList)
# subDataOutDir <- paste0(dataOutDir, sourceList)
  
 
  
  # Save the record:
  # ================

  # Check to see if the output folder exists. Create one if it doesn't.
  if ( i == 1){
  predictionTileFolder <- paste0(dataOutDir, sourceList, '/modelPredictionTimeseries')
  
    if (!file.exists(predictionTileFolder) ){
      dir.create(file.path(predictionTileFolder))
      }
  }

  # Save it:
  save(FullRecord, file=paste0(predictionTileFolder, '/predictionRecord_DaymetTile' , DaymetTiles[i], '_', Year, '.RData'))
}

```

```{r Write out prediction files for ArcGIS}

names(Predictions)[names(Predictions) == 'site'] <- 'FEATUREID'

Predictions <- merge(Predictions, predictionStats[,c('FEATUREID', 'StreamOrder')], by = 'FEATUREID', all.x = T, sort = F)

#Write out prediction files for ArcGIS:
#--------------------------------------
names(Predictions) <- c('FEATUREID', 'SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI', 'StreamOrder')

Predictions <- replace(Predictions, is.na(Predictions), -9999)

write.csv(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.csv'), row.names = F )
write.dbf(Predictions, file = paste0(dataOutDir, sourceList, '/BP_FullPredictions', Year, '.dbf'))

```

```{r Write out predictions for headwaters only (stream order 3 or less)}
streamOrderTrim <- Predictions

streamOrderTrim[ streamOrderTrim$StreamOrder > 3 , c('SpringBP', 'SummerBP', 'FallBP', 'SyncTemps', 'RiseSlope', 'RiseSloCI', 'FallSlope', 'FallSloCI', 'SummerMaxT', 'SumrMaxTCI')] <- -9999

write.csv(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.csv'), row.names = F )
write.dbf(streamOrderTrim, file = paste0(dataOutDir, sourceList, '/BP_HeadwatersPredictions', Year, '.dbf'))
```
