This script replaces covariate data for a site after visual inspection of site location resulting in changes to assigned NHD FeatureID or whether local or upstream data is indexed.

Notes:

- In the "siteChanges_(agency)" files. A "1" means yes and a "0"" means no.
In the "correctFeatureID"" column a 1 indicates to use the same featureID as before while any changes will be indicated by the new featureID. In the "localCatchment" column a 1 indicates to use the values for the local catchment while a 1 indicates to use the upstream catchment delineation values.

- The impoundment related data cannot be indexed for local stats because of how the original data is processed.                For now these get NA values. This could potentially be changed to zero values if we assume there are no impoundments in small (single catchment) headwaters.


rm(list=ls())

library(sp)
library(rgdal)
library(rgeos)
library(maptools)     # loads sp library too
library(chron)
library(ncdf)

baseDir <- 'C:/KPONEIL/GitHub/projects/'


#Load the function that indexes daymet tiles based on a lat/lon point:
source(paste0(baseDir, 'temperatureProject/code/functions/indexDaymetTileByLatLon.R'))
#==================================================================================================================
#                             Read in spatial data
#==================================================================================================================

proj4.NHD  <- "+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs"

#proj4.Daymet <- "+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=lonlat +no_defs "
#proj4.NHDLambert <- "+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83  +units=m +no_defs "

#Catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/NENY_NHDCatchment_Lambert.shp", proj4string=CRS(proj4.NHDLambert))
Catchments <- readShapePoly ( "C:/KPONEIL/USGS/NHDPlusV2/Modified Versions/NENY_NHDCatchment.shp", proj4string=CRS(proj4.NHD))

dataSource <- "VTFWS"

load(paste0(baseDir, 'temperatureProject/dataIn/delineatedCatchments/DelineatedCatchments_NHDPlus_NENY.RData'))
DelineatedCatchmentsMaster <- NENYDelineatedCatchments
MasterLength <- length(DelineatedCatchmentsMaster)





```{r Read in data}
# Read in the .csv for the site changes which indicates a 1 for OK, 0 for unknown issue, or the new FeatureID.
rm(list=ls())

# Define agency to edit
agency <- 'MAFW'

setwd(paste0('C:/KPONEIL/GitHub/projects/temperatureProject/dataIn/', agency))

# Sites to be changed
s <- read.csv(paste0('siteChanges_', agency, '.csv'))
s$site <- as.character(s$site)

# Specific sites to change
s1 <- s[which(s$correctFeatureID > 1 | s$localCatchment > 0 ),]

# Edit the "correctFeatureID" column to contain both changes and ones that will stay the same.
s1$correctFeatureID[s1$correctFeatureID == 1] <- s1$currentFeatureID[s1$correctFeatureID == 1]

# Dataframe to be edited
load(paste0('streamTempSitesObservedClimateData_', agency, '.RData'))
e <- masterData

```

```{r Replace columns that get local values}

# Sites that get local data:
#---------------------------
nL <- s1[s1$localCatchment == 1, c('site', 'correctFeatureID')]
colnames(nL) <- c('site', 'FEATUREID')
nL1 <- merge(nL, d[,c('site', 'Latitude', 'Longitude')], by = 'site', all.x = T, sort = F)

# Merge in new covariate data
nL2 <- merge(nL1, loc, by = 'FEATUREID', all.x = T, sort = F)

# Remove unused columns
nL3 <- nL2[,which(names(nL2) %in% names(d1))]

# Add empty columns because the impoundment data isn't applicable to local stats
missingNames <- names(nU3)[which(!names(nU3) %in% names(nL3))]
empty <- data.frame(array(NA, c(nrow(nL3), length(missingNames))))
colnames(empty) <- missingNames
nL4 <- cbind(nL3, empty)










```

```{r Replace columns}
# Don't want to replace Lat/Lon of the site with Lat/Lon of catchment
up  <- UpstreamStats[, - which(names(UpstreamStats) %in% c('Latitude', 'Longitude'))]
loc <- LocalStats   [, - which(names(LocalStats) %in% c('Latitude', 'Longitude'))]

# Remove old site covariate data
d1 <- d[!(d$site %in% s1$site), ]

#Replace sites that need new Upstream covariate data:
#====================================================

# Sites that get upstream data:
#------------------------------
nU <- s1[s1$localCatchment == 0, c('site', 'correctFeatureID')]
colnames(nU) <- c('site', 'FEATUREID')
nU1 <- merge(nU, d[,c('site', 'Latitude', 'Longitude')], by = 'site', all.x = T, sort = F)

# Merge in new covariate data
nU2 <- merge(nU1, up, by = 'FEATUREID', all.x = T, sort = F)

# Remove unused columns
nU3 <- nU2[,which(names(nU2) %in% names(d1))]


# Join new data together
newData <- rbind(nU3, nL4)

# Add a columnso we know which sites have had their data changed
newData$locationChange <- 'Y'
d1$locationChange <- 'N'

# Join new to existing keeper data
covariateData <- rbind(d1, newData)
```

```{r Save new dataframe}
# Save the new dataframe
save(covariateData, file = paste0('covariateData_', agency, '.RData'))
```


